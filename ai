#!/usr/bin/python

# Third party libraries
import sys
from rich import print as rprint
from rich.traceback import install

# Internal library
from lib.ai import command_handler
from lib.ai.arguments import argument_parser
from lib.ai import terminal
from lib.ai.session import interactive_session, passive_session, finish_reason_check
from lib.ai.user_input import initial_message
from lib.ai.user_input import choose_saved_chat
from lib.ai import io
from lib.ai import messages
from lib import config
from lib.desktop.clipboard import send_to_clipboard
from lib.openai import models

# This install function from 'rich' adds syntax highlighting to exceptions
install()


models.assert_openai_api_key()
text_model_name = config.get_text_model_name()
flags, commands, parameters = argument_parser()

if flags['debug']:
    rprint(f'flags: {flags}')
    rprint(f'commands: {commands}')
    rprint(f'parameters: {parameters}')

if commands['help']:
    terminal.print_title(text_model_name)
    command_handler.help_command(interactive=flags['interactive'])
    terminal.print_line()
    sys.exit(0)
if commands['list']:
    command_handler.list_command(commands['list'])
if commands['edit']:
    command_handler.edit_prompts(commands['edit'])
if commands['load']:
    loaded_chat = command_handler.load(commands['load'])
    rprint(loaded_chat)
    sys.exit(0)


model_list = [m['name'] for m in models.filter_models('gpt')]
if not text_model_name in model_list or flags['model']:
    text_model_name = models.choose_model(text_model_name, flags['model'])
    config.set_text_model_name(text_model_name)

prompt_name, prompt = command_handler.get_prompt(commands['prompt'])
if prompt:
    messages.add_user_content(prompt)
    messages.add_assistant_content('Understood')

file_content = command_handler.file_content(commands['file'])
if file_content:
    rprint(f'[yellow]File content loaded: {commands["file"]}[/]')
    messages.add_user_content(file_content)
    if not prompt:
        # If a prepared prompt has not been added, stop the message
        # from the API and ask the user for an initial message
        messages.add_assistant_content('How can I assist you?')

if not commands['query'] and not flags['interactive'] and not messages.ready_to_send():
    while not commands['query']:
        commands['query'] = initial_message()

if commands['query']:
    messages.add_user_content(commands['query'])

if flags['interactive']:
    if flags['verbose']:
        terminal.print_verbose(text_model_name, flags,
                              commands, parameters, prompt_name, None)
    interactive_session(text_model_name, flags, commands, parameters)

if flags['debug']:
    rprint(f'messages: {messages.chat}')

terminal.print_title(text_model_name, to_stderr=True)

session_data = passive_session(
    text_model_name, flags, commands, parameters)

finish_reason_check(session_data['finish_reason'])

if flags['verbose']:
    terminal.print_title(text_model_name)
    terminal.print_verbose(text_model_name, flags, commands,
                          parameters, prompt_name, session_data['tokens'])

send_to_clipboard(session_data['content'])
if flags['synchronous'] or flags['verbose']:
    print(session_data['content'])
