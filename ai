#!/usr/bin/python

# Python libraries
import argparse
import os
import subprocess
import sys
from pathlib import Path

# Third party libraries
from pick import pick
from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.styles import Style
from rich import print as rprint
from rich.color import Color
from rich.console import Console
from rich.rule import Rule
from rich.table import Table
from rich.traceback import install

# Internal library
from lib.args.ai_arguments import argument_parser
from lib.config import save_config, load_config
from lib.definitions import AI_HISTORY_PATH
from lib.desktop.clipboard import send_to_clipboard
from lib.openai import models
from lib.openai import prompts
from lib.openai import text

console = Console()
install()


models.assert_openai_api_key()
argflags = argument_parser()

if argflags['list']:
    if argflags['list'] == True:
        try:
            argflags['list'] = pick(
                ['prompts', 'models'], 'What would you like to list:', indicator='>')[0]
        except KeyboardInterrupt:
            sys.exit(0)
    if argflags['list'] == 'prompts':
        prompts.show_prompt_list()
    if argflags['list'] == 'models':
        models.show_model_list()
    sys.exit(0)


if argflags['edit']:
    prompts_path = prompts.get_prompts_path()
    rprint('[cyan]Opening prepared prompts path in VSCode[/]')
    rprint(f'[cyan]Path: "{prompts_path}"[/]')
    subprocess.run(['code', prompts_path])
    sys.exit(0)


session = PromptSession(history=FileHistory(AI_HISTORY_PATH))
config = load_config()
messages = list()


model_list = [m['name'] for m in models.filter_models('gpt')]
if not 'model' in config or not config['model'] in model_list or argflags['change_model']:
    config['model'] = models.choose_model()
    save_config(config)
model_name = config['model']


prompt_name = None
if argflags['prompt']:
    if len(argflags['prompt']) > 1:
        try:
            prompt_name = pick(
                argflags['prompt'], 'Choose a prepared prompt:', indicator='>')[0]
        except KeyboardInterrupt:
            sys.exit(0)
    if len(argflags['prompt']) == 1:
        prompt_name = argflags['prompt'][0]

    prompt = prompts.get_prompt_content(prompt_name)

    if prompt is None:
        rprint(f'Error: Prompt "{prompt_name}" does not exist')
        sys.exit(1)
    messages.append({'role': 'user', 'content': prompt})
    messages.append({'role': 'assistant', 'content': 'Understood'})


if argflags['file']:
    file_content = argflags['file'].read_text()
    messages.append({'role': 'user', 'content': file_content})
    messages.append({'role': 'assistant', 'content': 'How can I assist you?'})


if not argflags['query']:
    try:
        rprint(
            '[cyan]Type a message to start a chat, or a file path to send the contents:[/]')
        argflags['query'] = session.prompt('> ')
    except KeyboardInterrupt:
        sys.exit(0)


if argflags['query']:
    query = argflags['query']
    messages.append({'role': 'user', 'content': query})


def print_line():
    console.print(Rule(style='blue'))


def print_verbose_details(argflags, model_name, prompt_name, tokens, messages):
    rprint('[bold yellow]Session Details[/]')
    print_line()
    rprint('Command line arguments:')
    rprint(argflags)
    print_line()
    rprint('[yellow]Metadata[/]')
    print_line()
    rprint(f'[magenta]Model: [cyan]{model_name}[/]')
    rprint(f'[magenta]Prompt: [cyan]{prompt_name}[/]')
    if tokens:
        rprint(f'[magenta]Tokens:[/]')
        rprint(f'[magenta]  Prompt: [cyan]{tokens[0]}[/]')
        rprint(f'[magenta]  Completion: [cyan]{tokens[1]}[/]')
        rprint(f'[magenta]  Total: [cyan]{tokens[2]}[/]')
    print_line()
    rprint('[yellow]Messages[/]')
    print_line()
    for part in messages:
        rprint(f'[magenta]Role: {part["role"]}[/]')
        rprint(f'[cyan]{part["content"]}[/]')
        print_line()


def start_interactive_session(model_name, messages, argflags):
    def show_command_help():
        rprint(f'[cyan] Available commands:[/]')
        rprint(f'[cyan]   exit[/]')
        rprint(f'[cyan]   reset[/]')
        rprint(f'[cyan]   help[/]')
    print_line()
    rprint(f'[cyan] ChatGPT | {model_name}[/]')
    print_line()
    show_command_help()
    print_line()
    if argflags['verbose']:
        print_verbose_details(argflags, model_name,
                              prompt_name, None, messages)
    true_color = Color.parse('cyan').get_truecolor()
    hex_color = f'#{true_color[0]:02x}{true_color[1]:02x}{true_color[2]:02x}'
    style = Style.from_dict({'': hex_color})
    call_api = True
    while True:
        if len(messages) > 0 and call_api:
            response = text.call_gpt_async(
                model_name,
                messages,
                argflags['temperature'],
                argflags['frequency_penalty'],
                argflags['presence_penalty'],
                argflags['logit_bias'])
            messages.append(
                {'role': 'assistant', 'content': response['content']})
        call_api = True
        try:
            print_line()
            user_message = session.prompt('> ', style=style)
            print_line()
        except KeyboardInterrupt:
            sys.exit(0)
        if user_message.lower() == 'exit'.lower():
            sys.exit(0)
        if user_message.lower() == 'reset'.lower():
            messages = list()
            call_api = False
            rprint(f'[cyan] Session Reset | {model_name}')
            continue
        if user_message.lower() == 'help'.lower():
            show_command_help()
            call_api = False
            continue
        messages.append({'role': 'user', 'content': user_message})


if argflags['interactive']:
    start_interactive_session(model_name, messages, argflags)

sync_call_required = argflags['synchronous'] or argflags['verbose']
if sync_call_required:
    response = text.call_gpt_sync(
        model_name, messages, argflags['temperature'], argflags['frequency_penalty'], argflags['presence_penalty'],  argflags['logit_bias'])
    # print(response)
    finish_reason = response['choices'][0]['finish_reason']
    send_to_clipboard(response['choices'][0]['message']['content'])
else:
    response = text.call_gpt_async(
        model_name, messages, argflags['temperature'], argflags['frequency_penalty'], argflags['presence_penalty'],  argflags['logit_bias'])
    finish_reason = response['finish_reason']
    send_to_clipboard(response['content'])

if finish_reason != 'stop':
    known_reasons = {
        'length': 'Incomplete model output due to max_tokens parameter or token limit',
        'content_filter': 'Omitted content due to a flag from our content filters',
        'null': 'API response still in progress or incomplete'
    }
    if finish_reason in known_reasons.keys():
        rprint(f'[red]{known_reasons[finish_reason]}[/]')
    else:
        rprint(
            f'[red]Model output stopped for an unknown reason: {finish_reason}[/]')

if argflags['verbose']:
    print_line()
    rprint(f'[cyan] ChatGPT | {model_name}')
    print_line()
    tokens = (response['usage']['prompt_tokens'], response['usage']
              ['completion_tokens'], response['usage']['total_tokens'])
    print_verbose_details(argflags, model_name, prompt_name, tokens, messages)

if sync_call_required:
    print(response['choices'][0]['message']['content'])
