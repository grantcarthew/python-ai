Sends requests to ChatGPT using the OpenAI API.
The result is shown in the terminal and copied onto the clipboard.

An OpenAI API key is required:
export OPENAI_API_KEY=<your-private-key>

Prepared Prompts
----------------
You can preload prepared prompts to focus the AI.
Prepared prompts are Markdown documents in the "prompts" directory.
If you wish to use the prepared prompts, they must be the first
command in the terminal after 'ai'.

To work with prepard prompts:

ai list prompts - To list the existing prompts.
ai edit - Create, edit, or delete the prepared prompts.
ai -v <prompt> <query> - View the prompts and messages used.
ai python - Load the python prompt for the request.
ai py - Load the python prompt if there is only one.
ai aws - Show a list of aws prompts to choose from.

There is a footer.md file that is appended to the end of
every prepared prompt. Review its contents.

File Review
-----------
The 'command' can be a path to a file such as a Python script.

At any time you can pass three commands such as:

ai <prepared-prompt> <file-path> <your-query>
ai <prepared-prompt> <your-query> <file-path>
ai <file-path> <your-query>
ai <your-query> <file-path>
ai <file-path>

The file contents will be loaded and sent as a user message
along with the query if provided.

Interactive
-----------
If you start an interactive session with 'ai -i', you can have
longer conversations about a topic. The input prompt supports
history over multiple runs and CTRL-R for searching history.

The history file is located at: $HOME/.config/ai/history

Interactive mode supports the following commands:

  exit  - End the session
  reset - Remove messages and start a fresh
  help  - Show this command help

Here is an example:

ai -i "Lets have a chat!"

Hint: if you point to a file with the 'command' arguments,
you can then discuss the file.

ai -i ./some/file "Please review this file"

Synchronous
-----------
By default the 'ai' script responds with content asynchronously
displaying the response in the terminal as it comes in.

If you wish you can change this behaviour by using 'ai -s'.

Note: If you use '-v' for verbose output, synchronous
calls will be forced.

Select Model
------------
If run for the first time, you will be asked to select which
GPT model you would like to use. At the time of writing this
you will have options similar to the following:

gpt-4              - Latest of v4, best but expensive.
gpt-4-0314         - Snapshot of v4, don't use.
gpt-3.5-turbo-0301 - Snapshot of v3.5, don't use.
gpt-3.5-turbo      - Latest of v3.5, cost efficient.

Once you have selected, it is saved into a config file:
$HOME/.config/ai/config

If you wish to change the model, use the 'ai -c'.

ai -c "3.5 was a stupid answer, I'll try something else"

This will save the changes into the config file.

To see which model you are using, use 'ai -v'.

Optional Model Parameters
=========================

The following are parameters you can use to tweak the
way the GPT model works. They are not required however,
feel free to experiment with them.

Temperature
-----------
What sampling temperature to use, between 0.0 and 2.0.
Higher values like 0.8 will make the output more random,
while lower values like 0.2 will make it more focused
and deterministic.

The default temperature is 0.

Try this:

ai --temperature 0 "Be creative, one short paragraph"
ai --temperature 1 "Be creative, one short paragraph"
ai --temperature 2 "Be creative, one short paragraph"

Presence Penalty
----------------
What presence penalty to use, between -2.0 to 2.0.
Positive values penalize new tokens based on whether
they appear in the text so far, increasing the model's
likelihood to talk about new topics.

The default presence penalty is 0.

Frequency Penalty
-----------------
What frequency penalty to use, between -2.0 and 2.0.
Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the
model's likelihood to repeat the same line verbatim.

The default frequency penalty is 0.

Logit Bias
----------
Modify the likelihood of specified tokens appearing in
the completion text.

Mathematically, the bias is added to the logits generated
by the model prior to sampling. The exact effect will vary
per model, but values between -1 and 1 should decrease or
increase likelihood of selection; values like -100 or 100
should result in a ban or exclusive selection of the relevant
token.

To pass values into this option, use a comma separated list:

ai "Say the word 'Yes'" --logit-bias '9642:-100'

In this example '6942' is a token id representing 'Yes'.

You will need to determine the token ids via:
https://github.com/openai/tiktoken
